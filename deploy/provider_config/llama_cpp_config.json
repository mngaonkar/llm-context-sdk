{
            "config_type": "llm_provider",
            "provider_name": "llama_cpp",
            "model": "llava",
            "base_url": "http://localhost:11434",
            "temperature": 0.6
}